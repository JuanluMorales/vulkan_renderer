// Copyright (C) 2022, Christoph Peters, Karlsruhe Institute of Technology
//
// This source code file is licensed under both the three-clause BSD license
// and the GPLv3. You may select, at your option, one of the two licenses. The
// corresponding license headers follow:
//
//
// Three-clause BSD license:
//
//  Redistribution and use in source and binary forms, with or without
//  modification, are permitted provided that the following conditions are met:
//
//  1. Redistributions of source code must retain the above copyright notice,
//     this list of conditions and the following disclaimer.
//
//  2. Redistributions in binary form must reproduce the above copyright
//     notice, this list of conditions and the following disclaimer in the
//     documentation and/or other materials provided with the distribution.
//
//  3. Neither the name of the copyright holder nor the names of its
//     contributors may be used to endorse or promote products derived from
//     this software without specific prior written permission.
//
//  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
//  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
//  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
//  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
//  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
//  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
//  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
//  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
//  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
//  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
//  POSSIBILITY OF SUCH DAMAGE.
//
//
// GPLv3:
//
//  This program is free software: you can redistribute it and/or modify
//  it under the terms of the GNU General Public License as published by
//  the Free Software Foundation, either version 3 of the License, or
//  (at your option) any later version.
//
//  This program is distributed in the hope that it will be useful,
//  but WITHOUT ANY WARRANTY; without even the implied warranty of
//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//  GNU General Public License for more details.
//
//  You should have received a copy of the GNU General Public License
//  along with this program.  If not, see <https://www.gnu.org/licenses/>.


#include "blend_attribute_codec.glsl"


//! Swaps a[lhs] and a[rhs] iff a[lhs] > a[rhs]. Useful for sorting networks.
void cmp(inout uint a[ENTRY_COUNT], uint lhs, uint rhs) {
	bool swap = a[lhs] > a[rhs];
	uint a_lhs = a[lhs];
	a[lhs] = swap ? a[rhs] : a_lhs;
	a[rhs] = swap ? a_lhs : a[rhs];
}


//! Sorts the given sequence in ascending order in place
void sort(inout uint a[ENTRY_COUNT]) {
	// These are optimal sorting networks (minimal number of comparisons and
	// probably minimal depth for that comparison count), mostly generated by
	// Batcher's algorithm or taken from figures 49 and 51 of:
	// Donald E. Knuth, 1998, The Art of Computer Programming, Volume 3 -
    // Sorting and Searching, 2nd Edition
#if ENTRY_COUNT == 2
	cmp(a, 0, 1);
#elif ENTRY_COUNT == 3
	cmp(a, 0, 1);  cmp(a, 0, 2);  cmp(a, 1, 2);
#elif ENTRY_COUNT == 4
	cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 0, 2);  cmp(a, 1, 3);  cmp(a, 1, 2);
#elif ENTRY_COUNT == 5
	cmp(a, 0, 1);  cmp(a, 3, 4);  cmp(a, 0, 2);  cmp(a, 1, 2);  cmp(a, 0, 4);  cmp(a, 2, 4);  cmp(a, 1, 3);  cmp(a, 0, 1);  cmp(a, 2, 3);
#elif ENTRY_COUNT == 6
	cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 4, 5);  cmp(a, 0, 2);  cmp(a, 3, 5);  cmp(a, 1, 4);  cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 4, 5);  cmp(a, 1, 2);  cmp(a, 3, 4);  cmp(a, 2, 3);
#elif ENTRY_COUNT == 7
	cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 4, 5);  cmp(a, 4, 6);  cmp(a, 0, 2);  cmp(a, 1, 3);  cmp(a, 1, 2);  cmp(a, 5, 6);  cmp(a, 0, 4);  cmp(a, 1, 5);  cmp(a, 2, 6);  cmp(a, 2, 4);  cmp(a, 3, 5);  cmp(a, 1, 2);  cmp(a, 3, 4);  cmp(a, 5, 6);
#elif ENTRY_COUNT == 8
	cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 4, 5);  cmp(a, 6, 7);  cmp(a, 0, 2);  cmp(a, 1, 3);  cmp(a, 4, 6);  cmp(a, 5, 7);  cmp(a, 5, 6);  cmp(a, 1, 2);  cmp(a, 0, 4);  cmp(a, 3, 7);  cmp(a, 2, 6);  cmp(a, 1, 5);  cmp(a, 2, 4);  cmp(a, 3, 5);  cmp(a, 1, 2);  cmp(a, 3, 4);  cmp(a, 5, 6);
#elif ENTRY_COUNT == 9
	cmp(a, 1, 2);  cmp(a, 3, 4);  cmp(a, 5, 6);  cmp(a, 7, 8);  cmp(a, 0, 1);  cmp(a, 2, 4);  cmp(a, 6, 8);  cmp(a, 5, 7);  cmp(a, 0, 3);  cmp(a, 4, 8);  cmp(a, 1, 2);  cmp(a, 6, 7);  cmp(a, 3, 7);  cmp(a, 1, 6);  cmp(a, 0, 5);  cmp(a, 2, 4);  cmp(a, 4, 7);  cmp(a, 2, 6);  cmp(a, 3, 5);  cmp(a, 1, 3);  cmp(a, 4, 6);  cmp(a, 7, 8);  cmp(a, 2, 5);  cmp(a, 2, 3);  cmp(a, 4, 5);
#elif ENTRY_COUNT == 10
	cmp(a, 4, 9);  cmp(a, 3, 8);  cmp(a, 2, 7);  cmp(a, 1, 6);  cmp(a, 0, 5);  cmp(a, 1, 4);  cmp(a, 6, 9);  cmp(a, 0, 3);  cmp(a, 5, 8);  cmp(a, 0, 2);  cmp(a, 3, 6);  cmp(a, 7, 9);  cmp(a, 0, 1);  cmp(a, 2, 4);  cmp(a, 5, 7);  cmp(a, 8, 9);  cmp(a, 4, 6);  cmp(a, 1, 2);  cmp(a, 7, 8);  cmp(a, 3, 5);  cmp(a, 2, 5);  cmp(a, 6, 8);  cmp(a, 1, 3);  cmp(a, 4, 7);  cmp(a, 2, 3);  cmp(a, 6, 7);  cmp(a, 3, 4);  cmp(a, 5, 6);  cmp(a, 4, 5);
#elif ENTRY_COUNT == 11
	cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 4, 5);  cmp(a, 6, 7);  cmp(a, 8, 9);  cmp(a, 1, 3);  cmp(a, 5, 7);  cmp(a, 8, 10);  cmp(a, 0, 2);  cmp(a, 4, 6);  cmp(a, 1, 6);  cmp(a, 2, 5);  cmp(a, 0, 4);  cmp(a, 7, 9);  cmp(a, 3, 10);  cmp(a, 1, 8);  cmp(a, 9, 10);  cmp(a, 3, 7);  cmp(a, 2, 4);  cmp(a, 6, 9);  cmp(a, 5, 8);  cmp(a, 4, 7);  cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 1, 2);  cmp(a, 3, 6);  cmp(a, 7, 8);  cmp(a, 4, 5);  cmp(a, 3, 5);  cmp(a, 6, 7);  cmp(a, 8, 9);  cmp(a, 2, 4);  cmp(a, 3, 4);  cmp(a, 5, 6);  cmp(a, 7, 8);
#elif ENTRY_COUNT == 12
	cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 4, 5);  cmp(a, 6, 7);  cmp(a, 8, 9);  cmp(a, 10, 11);  cmp(a, 0, 2);  cmp(a, 1, 3);  cmp(a, 4, 6);  cmp(a, 5, 7);  cmp(a, 8, 10);  cmp(a, 9, 11);  cmp(a, 1, 2);  cmp(a, 5, 6);  cmp(a, 9, 10);  cmp(a, 0, 4);  cmp(a, 1, 5);  cmp(a, 6, 10);  cmp(a, 7, 11);  cmp(a, 2, 6);  cmp(a, 3, 7);  cmp(a, 4, 8);  cmp(a, 5, 9);  cmp(a, 1, 5);  cmp(a, 6, 10);  cmp(a, 7, 11);  cmp(a, 0, 4);  cmp(a, 3, 8);  cmp(a, 1, 4);  cmp(a, 2, 3);  cmp(a, 7, 10);  cmp(a, 8, 9);  cmp(a, 2, 4);  cmp(a, 3, 5);  cmp(a, 6, 8);  cmp(a, 7, 9);  cmp(a, 3, 4);  cmp(a, 5, 6);  cmp(a, 7, 8);
#endif
}


//! Returns the factorial of n. n must be at most 12 because otherwise the
//! factorial does not fit into 32 bits.
uint factorial(uint n) {
	const uint factorials[13] = { 1, 1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800, 39916800, 479001600 };
	return factorials[n];
}


//! Constructs the permutation with the given Lehmer code by writing unique
//! indices from 0 to ENTRY_COUNT - 1 to permutation.
void get_permutation(out uint permutation[ENTRY_COUNT], uint lehmer) {
	permutation[ENTRY_COUNT - 1] = 0;
	// The iteration variable is the number of available choices for each entry
	[[unroll]]
	for (uint i = 2; i < ENTRY_COUNT + 1; ++i) {
		// Determine the choice
		uint old_lehmer = lehmer;
		lehmer /= i;
		uint lehmer_digit = old_lehmer - lehmer * i;
		permutation[ENTRY_COUNT - i] = lehmer_digit;
		// Avoid collisions among indices
		for (uint j = 1 + ENTRY_COUNT - i; j != ENTRY_COUNT; ++j)
			permutation[j] += (permutation[j] >= lehmer_digit) ? 1 : 0;
	}
}


/*! Constructs a Lehmer code from the given permutation.
	\param permutation The indices that the permutation maps to. Only the
		lowest four bits of each entry are considered.
	\return The Lehmer code.*/
uint get_lehmer(uint permutation[ENTRY_COUNT]) {
	// The first step is simple
	uint lehmer = (permutation[0] & 0xf) * factorial(ENTRY_COUNT - 1);
	// This is a bit field indicating which indices were not encountered yet
	uint unseen = bitfieldInsert(0xffff, 0x0, int(permutation[0] & 0xf), 1);
	[[unroll]]
	for (uint i = 1; i < ENTRY_COUNT - 1; ++i) {
		int permutation_i = int(permutation[i] & 0xf);
		// The index of permutation_i among the unseen indices
		uint lehmer_digit = bitCount(bitfieldExtract(unseen, 0, permutation_i));
		lehmer += factorial(ENTRY_COUNT - 1 - i) * lehmer_digit;
		unseen = bitfieldInsert(unseen, 0x0, permutation_i, 1);
	}
	return lehmer;
}


//! Permutes the given numbers (each less than 2^28) in place using the
//! inverse of the permutation with the given Lehmer code
void permute_inverse(inout uint a[ENTRY_COUNT], uint lehmer) {
	// Construct the permutation with the given Lehmer code
	uint permutation[ENTRY_COUNT];
	get_permutation(permutation, lehmer);
	// Place permutation indices in the high bits
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT; ++i)
		a[i] += 0x10000000 * permutation[i];
	// Now sort the sequence. That effectively applies the inverse permutation.
	sort(a);
	// Discard the indices again
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT; ++i)
		a[i] &= 0x0fffffff;
}


//! Sorts the given numbers (each less than 2^28) in ascending order in place.
//! It is a stable sort.
//! \return The Lehmer code of the permutation that was applied to the numbers.
uint stable_sort(inout uint a[ENTRY_COUNT]) {
	// Place indices in the low bits
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT; ++i)
		a[i] = 0x10 * a[i] + i;
	// Apply a sorting network
	sort(a);
	// Construct the Lehmer code from the indices
	uint lehmer = get_lehmer(a);
	// Discard the indices again
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT; ++i)
		a[i] = bitfieldExtract(a[i], 4, 28);
	return lehmer;
}


//! \return The number of different tuple indices that can be encoded by the
//!		given codec.
uint get_tuple_index_count(blend_attribute_codec_t codec) {
	uint tuple_index_count = codec.payload_value_count_over_factorial;
	tuple_index_count *= factorial(ENTRY_COUNT);
	// Divide by the extra value counts
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT; ++i)
		tuple_index_count /= codec.extra_value_counts[i];
	return tuple_index_count;
}


/*! \return The number of weights that can be encoded before the code needs
		more than 32 bits. The extra bits and the tuple index are encoded
		before that.
	\note codec should be a compile-time constant so that this function takes
		no run time.*/
uint get_32_bit_weight_count(blend_attribute_codec_t codec) {
	// Figure out how many weights have to be extracted before code[1] == 0.
	// This should run completely at compile time.
	uint power_msb = 0;
	uint power_lsb = codec.payload_value_count_over_factorial;
	uint weight_count_32 = 0;
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT; ++i) {
		uint overflow;
		umulExtended(power_lsb, codec.weight_value_count, overflow, power_lsb);
		power_msb = power_msb * codec.weight_value_count + overflow;
		if (power_msb > 1 || (power_msb == 1 && power_lsb > 0))
			break;
		++weight_count_32;
	}
	return weight_count_32;
}


/*! Encodes blend weights and a tuple index for the table of bone indices into
	up to 64 bits.
	\param weights Non-negative blend weights sorted in ascending order. Their
		sum must be one.
	\param tuple_index An index into a table of blend index tuples.
	\param codec Parameters for the coding scheme. Should be a compile-time
		constant.
	\return The code for the weights and the tuple index. If the codec takes
		less than 64 bits, the most significant bits of result[1] are 0. Same
		for result[0] if it takes less than 32 bits.*/
uvec2 compress_blend_attributes(float weights[ENTRY_COUNT + 1], uint tuple_index, blend_attribute_codec_t codec) {
	// We mostly do work per weight
	float prefix = 0.0f;
	uint quantized[ENTRY_COUNT];
	uint extra[ENTRY_COUNT];
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT; ++i) {
		// Transform the weights so that they fill the unit cube (once we
		// permutate them)
		float cube_weight = (ENTRY_COUNT + 1 - i) * weights[i] + prefix;
		prefix += weights[i];
		// Quantize to the requested precision and apply an offset to ensure a
		// strict ordering
		const uint value_count = (codec.weight_value_count - ENTRY_COUNT) * codec.extra_value_counts[i];
		const uint offset = (i + 1) * codec.extra_value_counts[i] - 1;
		uint combined = uint(float(value_count) * cube_weight + (offset + 0.5f));
		// Separate the extra precision from the base precision
		quantized[i] = combined / codec.extra_value_counts[i];
		extra[i] = combined - codec.extra_value_counts[i] * quantized[i];
	}
	// Pack the tuple index and all extra precision into one integer
	uint payload = tuple_index;
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT; ++i)
		payload = payload * codec.extra_value_counts[i] + extra[i];
	// Grab high bits from the payload to store them separately
	uint payload_quotient = payload / factorial(ENTRY_COUNT);
	uint lehmer = payload - factorial(ENTRY_COUNT) * payload_quotient;
	// Encode part of the payload into the permutation
	permute_inverse(quantized, lehmer);
	// Pack it all into two uints
	uvec2 code = uvec2(payload_quotient, 0);
	const uint weight_count_32 = get_32_bit_weight_count(codec);
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT; ++i) {
		[[flatten]]
		if (i < weight_count_32)
			code[0] = code[0] * codec.weight_value_count + quantized[i];
		else {
			// Multiply by codec.weight_value_count
			uint overflow;
			umulExtended(code[0], codec.weight_value_count, overflow, code[0]);
			code[1] = code[1] * codec.weight_value_count + overflow;
			// Add the next weight
			code[0] = uaddCarry(code[0], quantized[i], overflow);
			code[1] += overflow;
		}
	}
	return code;
}


/*! Division with remainder of a 64-bit unsigned integer by a 16-bit unsigned
	integer.
	\param msb The 32 most significant bits of the dividend.
	\param lsb The 32 least significant bits of the dividend.
	\param divisor_16 The divisor. Less than 2^16=65536.
	\param out_msb The 32 most significant bits of the quotient.
	\param out_lsb The 32 least significant bits of the quotient.
	\return The remainder of the division.*/
uint udiv_extended(uint msb, uint lsb, uint divisor_16, out uint out_msb, out uint out_lsb) {
	// Division with remainder for the most significant bits
	uint msb_quotient = msb / divisor_16;
	uint msb_remainder = msb - msb_quotient * divisor_16;
	// Merging the remainder with the 16 middle bits followed by another
	// division with remainder
	uint middle = bitfieldInsert(bitfieldExtract(lsb, 16, 16), msb_remainder, 16, 16);
	uint middle_quotient = middle / divisor_16;
	uint middle_remainder = middle - middle_quotient * divisor_16;
	// Same for the least significant 16 bits
	uint least = bitfieldInsert(lsb, middle_remainder, 16, 16);
	uint least_quotient = least / divisor_16;
	uint least_remainder = least - least_quotient * divisor_16;
	// Put together the quotient
	out_msb = msb_quotient;
	out_lsb = bitfieldInsert(least_quotient, middle_quotient, 16, 16);
	return least_remainder;
}


/*! Inverse of compress_blend_attributes().
	\param out_weights The decoded weights (with some rounding error).
	\param out_valid Set to true if the given code may have been produced by
		compress_blend_attributes(). Useful for debugging, ignore it otherwise.
	\param code Output of compress_blend_attributes().
	\param codec Parameters for the coding scheme. Should be a compile-time
		constant.
	\return The tuple index that was passed to compress_blend_attributes().*/
uint decompress_blend_attributes(out float out_weights[ENTRY_COUNT + 1], out bool out_valid, uvec2 code, blend_attribute_codec_t codec) {
	// Grab the quantized weights in reverse order
	const uint weight_count_32 = get_32_bit_weight_count(codec);
	uint quantized[ENTRY_COUNT];
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT; ++i) {
		// Extract the appropriate fractional bits and shift the code
		[[flatten]]
		if (ENTRY_COUNT - i <= weight_count_32) {
			uint old_code = code[0];
			code[0] /= codec.weight_value_count;
			quantized[ENTRY_COUNT - 1 - i] = old_code - code[0] * codec.weight_value_count;
		}
		else
			quantized[ENTRY_COUNT - 1 - i] = udiv_extended(code[1], code[0], codec.weight_value_count, code[1], code[0]);
	}
	// Grab the parts of the payload, which are not packed into the permutation
	uint payload_quotient = code[0];
	// Recover the Lehmer code while sorting the weights
	uint lehmer = stable_sort(quantized);
	// Assemble the payload
	uint payload = payload_quotient * factorial(ENTRY_COUNT) + lehmer;
	// Extract extra bits from the payload
	uint extra[ENTRY_COUNT];
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT; ++i) {
		uint old_payload = payload;
		payload /= codec.extra_value_counts[ENTRY_COUNT - 1 - i];
		extra[ENTRY_COUNT - 1 - i] = old_payload - payload * codec.extra_value_counts[ENTRY_COUNT - 1 - i];
	}
	// What remains of the payload is the tuple index
	uint tuple_index = payload;
	// Check if the code is legal (no two values are identical and all values
	// are in range)
	out_valid = true;
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT - 1; ++i)
		out_valid = (out_valid && quantized[i] != quantized[i + 1]);
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT; ++i) {
		uint combined = quantized[i] * codec.extra_value_counts[i] + extra[i];
		out_valid = (out_valid
			&& combined >= (i + 1) * codec.extra_value_counts[i] - 1
			&& combined < (codec.weight_value_count + i + 1 - ENTRY_COUNT) * codec.extra_value_counts[i]);
	}
	out_valid = (out_valid && payload_quotient < codec.payload_value_count_over_factorial);
	// Dequantize the weights and undo the transform
	float prefix = 0.0f;
	[[unroll]]
	for (uint i = 0; i != ENTRY_COUNT; ++i) {
		// Dequantize to get weights in the simplex that fills the cube
		uint combined = quantized[i] * codec.extra_value_counts[i] + extra[i];
		combined -= codec.extra_value_counts[i] * (i + 1) - 1;
		const float spacing = 1.0f / float((codec.weight_value_count - ENTRY_COUNT) * codec.extra_value_counts[i]);
		float cube_weight = spacing * float(combined);
		// Transform back to common blend weights
		out_weights[i] = (1.0f / (ENTRY_COUNT + 1 - i)) * cube_weight - prefix;
		prefix += (1.0f / ((ENTRY_COUNT + 1 - i) * (ENTRY_COUNT - i))) * cube_weight;
	}
	// It is not immediately obvious, but prefix is the sum of all weights
	// except the greatest one now
	out_weights[ENTRY_COUNT] = 1.0f - prefix;
	return tuple_index;
}
