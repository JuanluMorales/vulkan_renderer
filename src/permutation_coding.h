// Copyright (C) 2022, Christoph Peters, Karlsruhe Institute of Technology
//
// This source code file is licensed under both the three-clause BSD license
// and the GPLv3. You may select, at your option, one of the two licenses. The
// corresponding license headers follow:
//
//
// Three-clause BSD license:
//
//  Redistribution and use in source and binary forms, with or without
//  modification, are permitted provided that the following conditions are met:
//
//  1. Redistributions of source code must retain the above copyright notice,
//     this list of conditions and the following disclaimer.
//
//  2. Redistributions in binary form must reproduce the above copyright
//     notice, this list of conditions and the following disclaimer in the
//     documentation and/or other materials provided with the distribution.
//
//  3. Neither the name of the copyright holder nor the names of its
//     contributors may be used to endorse or promote products derived from
//     this software without specific prior written permission.
//
//  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
//  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
//  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
//  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
//  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
//  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
//  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
//  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
//  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
//  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
//  POSSIBILITY OF SUCH DAMAGE.
//
//
// GPLv3:
//
//  This program is free software: you can redistribute it and/or modify
//  it under the terms of the GNU General Public License as published by
//  the Free Software Foundation, either version 3 of the License, or
//  (at your option) any later version.
//
//  This program is distributed in the hope that it will be useful,
//  but WITHOUT ANY WARRANTY; without even the implied warranty of
//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
//  GNU General Public License for more details.
//
//  You should have received a copy of the GNU General Public License
//  along with this program.  If not, see <https://www.gnu.org/licenses/>.


#include "blend_attribute_codec.h"
#include <stdint.h>

//! This implementation supports at most 13 bones per vertex/12 stored weights
#define MAX_ENTRY_COUNT 12


//! Returns the factorial of n. n must be at most 12 because otherwise the
//! factorial does not fit into 32 bits.
static inline uint32_t factorial(uint32_t n) {
	const uint32_t factorials[13] = { 1, 1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800, 39916800, 479001600 };
	return factorials[n];
}


//! Implementation of bitfieldInsert() in GLSL
static inline uint32_t bitfield_insert(uint32_t base, uint32_t insert, int offset, int bits) {
	uint32_t mask = ((1ul << bits) - 1ul) << offset;
	return (base & ~mask) | ((insert << offset) & mask);
}


//! Implementation of bitfieldExtract() in GLSL
static inline uint32_t bitfield_extract(uint32_t value, int offset, int bits) {
	return (value >> offset) & ((1ul << bits) - 1ul);
}


//! Implementation of bitCount() in GLSL
static inline uint32_t bit_count(uint32_t field) {
	// Could be optimized more or replaced by some intrinsic
	uint32_t count = 0;
	for (uint32_t i = 0; i != 32; ++i) {
		count += field & 1;
		field >>= 1;
	}
	return count;
}


//! Swaps a[lhs] and a[rhs] iff a[lhs] > a[rhs]. Useful for sorting networks.
static inline void cmp(uint32_t* a, uint32_t lhs, uint32_t rhs) {
	int swap = (a[lhs] > a[rhs]);
	uint32_t a_lhs = a[lhs];
	a[lhs] = swap ? a[rhs] : a_lhs;
	a[rhs] = swap ? a_lhs : a[rhs];
}


//! Sorts the given sequence in ascending order in place
static inline void sort(uint32_t a[MAX_ENTRY_COUNT], uint32_t entry_count) {
	// These are optimal sorting networks (minimal number of comparisons and
	// probably minimal depth for that comparison count), mostly generated by
	// Batcher's algorithm or taken from figures 49 and 51 of:
	// Donald E. Knuth, 1998, The Art of Computer Programming, Volume 3 -
    // Sorting and Searching, 2nd Edition
	switch (entry_count) {
	case 2:
		cmp(a, 0, 1);
		break;
	case 3:
		cmp(a, 0, 1);  cmp(a, 0, 2);  cmp(a, 1, 2);
		break;
	case 4:
		cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 0, 2);  cmp(a, 1, 3);  cmp(a, 1, 2);
		break;
	case 5:
		cmp(a, 0, 1);  cmp(a, 3, 4);  cmp(a, 0, 2);  cmp(a, 1, 2);  cmp(a, 0, 4);  cmp(a, 2, 4);  cmp(a, 1, 3);  cmp(a, 0, 1);  cmp(a, 2, 3);
		break;
	case 6:
		cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 4, 5);  cmp(a, 0, 2);  cmp(a, 3, 5);  cmp(a, 1, 4);  cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 4, 5);  cmp(a, 1, 2);  cmp(a, 3, 4);  cmp(a, 2, 3);
		break;
	case 7:
		cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 4, 5);  cmp(a, 4, 6);  cmp(a, 0, 2);  cmp(a, 1, 3);  cmp(a, 1, 2);  cmp(a, 5, 6);  cmp(a, 0, 4);  cmp(a, 1, 5);  cmp(a, 2, 6);  cmp(a, 2, 4);  cmp(a, 3, 5);  cmp(a, 1, 2);  cmp(a, 3, 4);  cmp(a, 5, 6);
		break;
	case 8:
		cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 4, 5);  cmp(a, 6, 7);  cmp(a, 0, 2);  cmp(a, 1, 3);  cmp(a, 4, 6);  cmp(a, 5, 7);  cmp(a, 5, 6);  cmp(a, 1, 2);  cmp(a, 0, 4);  cmp(a, 3, 7);  cmp(a, 2, 6);  cmp(a, 1, 5);  cmp(a, 2, 4);  cmp(a, 3, 5);  cmp(a, 1, 2);  cmp(a, 3, 4);  cmp(a, 5, 6);
		break;
	case 9:
		cmp(a, 1, 2);  cmp(a, 3, 4);  cmp(a, 5, 6);  cmp(a, 7, 8);  cmp(a, 0, 1);  cmp(a, 2, 4);  cmp(a, 6, 8);  cmp(a, 5, 7);  cmp(a, 0, 3);  cmp(a, 4, 8);  cmp(a, 1, 2);  cmp(a, 6, 7);  cmp(a, 3, 7);  cmp(a, 1, 6);  cmp(a, 0, 5);  cmp(a, 2, 4);  cmp(a, 4, 7);  cmp(a, 2, 6);  cmp(a, 3, 5);  cmp(a, 1, 3);  cmp(a, 4, 6);  cmp(a, 7, 8);  cmp(a, 2, 5);  cmp(a, 2, 3);  cmp(a, 4, 5);
		break;
	case 10:
		cmp(a, 4, 9);  cmp(a, 3, 8);  cmp(a, 2, 7);  cmp(a, 1, 6);  cmp(a, 0, 5);  cmp(a, 1, 4);  cmp(a, 6, 9);  cmp(a, 0, 3);  cmp(a, 5, 8);  cmp(a, 0, 2);  cmp(a, 3, 6);  cmp(a, 7, 9);  cmp(a, 0, 1);  cmp(a, 2, 4);  cmp(a, 5, 7);  cmp(a, 8, 9);  cmp(a, 4, 6);  cmp(a, 1, 2);  cmp(a, 7, 8);  cmp(a, 3, 5);  cmp(a, 2, 5);  cmp(a, 6, 8);  cmp(a, 1, 3);  cmp(a, 4, 7);  cmp(a, 2, 3);  cmp(a, 6, 7);  cmp(a, 3, 4);  cmp(a, 5, 6);  cmp(a, 4, 5);
		break;
	case 11:
		cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 4, 5);  cmp(a, 6, 7);  cmp(a, 8, 9);  cmp(a, 1, 3);  cmp(a, 5, 7);  cmp(a, 8, 10);  cmp(a, 0, 2);  cmp(a, 4, 6);  cmp(a, 1, 6);  cmp(a, 2, 5);  cmp(a, 0, 4);  cmp(a, 7, 9);  cmp(a, 3, 10);  cmp(a, 1, 8);  cmp(a, 9, 10);  cmp(a, 3, 7);  cmp(a, 2, 4);  cmp(a, 6, 9);  cmp(a, 5, 8);  cmp(a, 4, 7);  cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 1, 2);  cmp(a, 3, 6);  cmp(a, 7, 8);  cmp(a, 4, 5);  cmp(a, 3, 5);  cmp(a, 6, 7);  cmp(a, 8, 9);  cmp(a, 2, 4);  cmp(a, 3, 4);  cmp(a, 5, 6);  cmp(a, 7, 8);
		break;
	case 12:
		cmp(a, 0, 1);  cmp(a, 2, 3);  cmp(a, 4, 5);  cmp(a, 6, 7);  cmp(a, 8, 9);  cmp(a, 10, 11);  cmp(a, 0, 2);  cmp(a, 1, 3);  cmp(a, 4, 6);  cmp(a, 5, 7);  cmp(a, 8, 10);  cmp(a, 9, 11);  cmp(a, 1, 2);  cmp(a, 5, 6);  cmp(a, 9, 10);  cmp(a, 0, 4);  cmp(a, 1, 5);  cmp(a, 6, 10);  cmp(a, 7, 11);  cmp(a, 2, 6);  cmp(a, 3, 7);  cmp(a, 4, 8);  cmp(a, 5, 9);  cmp(a, 1, 5);  cmp(a, 6, 10);  cmp(a, 7, 11);  cmp(a, 0, 4);  cmp(a, 3, 8);  cmp(a, 1, 4);  cmp(a, 2, 3);  cmp(a, 7, 10);  cmp(a, 8, 9);  cmp(a, 2, 4);  cmp(a, 3, 5);  cmp(a, 6, 8);  cmp(a, 7, 9);  cmp(a, 3, 4);  cmp(a, 5, 6);  cmp(a, 7, 8);
		break;
	default:
		break;
	}
}


//! Constructs the permutation with the given Lehmer code by writing unique
//! indices from 0 to entry_count - 1 to permutation.
static inline void get_permutation(uint32_t permutation[MAX_ENTRY_COUNT], uint32_t lehmer, uint32_t entry_count) {
	permutation[entry_count - 1] = 0;
	// The iteration variable is the number of available choices for each entry
	for (uint32_t i = 2; i < entry_count + 1; ++i) {
		// Determine the choice
		uint32_t old_lehmer = lehmer;
		lehmer /= i;
		uint32_t lehmer_digit = old_lehmer - lehmer * i;
		permutation[entry_count - i] = lehmer_digit;
		// Avoid collisions among indices
		for (uint32_t j = 1 + entry_count - i; j != entry_count; ++j)
			permutation[j] += (permutation[j] >= lehmer_digit) ? 1 : 0;
	}
}


/*! Constructs a Lehmer code from the given permutation.
	\param permutation The indices that the permutation maps to. Only the
		lowest four bits of each entry are considered.
	\return The Lehmer code.*/
static inline uint32_t get_lehmer(uint32_t permutation[MAX_ENTRY_COUNT], uint32_t entry_count) {
	// The first step is simple
	uint32_t lehmer = (permutation[0] & 0xf) * factorial(entry_count - 1);
	// This is a bit field indicating which indices were not encountered yet
	uint32_t unseen = bitfield_insert(0xffff, 0x0, (int) (permutation[0] & 0xf), 1);
	for (uint32_t i = 1; i < entry_count - 1; ++i) {
		int permutation_i = (int) (permutation[i] & 0xf);
		// The index of permutation_i among the unseen indices
		uint32_t lehmer_digit = bit_count(bitfield_extract(unseen, 0, permutation_i));
		lehmer += factorial(entry_count - 1 - i) * lehmer_digit;
		unseen = bitfield_insert(unseen, 0x0, permutation_i, 1);
	}
	return lehmer;
}


//! Permutes the given numbers (each less than 2^28) in place using the
//! inverse of the permutation with the given Lehmer code
static inline void permute_inverse(uint32_t a[MAX_ENTRY_COUNT], uint32_t lehmer, uint32_t entry_count) {
	// Construct the permutation with the given Lehmer code
	uint32_t permutation[MAX_ENTRY_COUNT];
	get_permutation(permutation, lehmer, entry_count);
	// Place permutation indices in the high bits
	for (uint32_t i = 0; i != entry_count; ++i)
		a[i] += 0x10000000 * permutation[i];
	// Now sort the sequence. That effectively applies the inverse permutation.
	sort(a, entry_count);
	// Discard the indices again
	for (uint32_t i = 0; i != entry_count; ++i)
		a[i] &= 0x0fffffff;
}


//! Sorts the given numbers (each less than 2^28) in ascending order in place.
//! It is a stable sort.
//! \return The Lehmer code of the permutation that was applied to the numbers.
static inline uint32_t stable_sort(uint32_t a[MAX_ENTRY_COUNT], uint32_t entry_count) {
	// Place indices in the low bits
	for (uint32_t i = 0; i != entry_count; ++i)
		a[i] = 0x10 * a[i] + i;
	// Apply a sorting network
	sort(a, entry_count);
	// Construct the Lehmer code from the indices
	uint32_t lehmer = get_lehmer(a, entry_count);
	// Discard the indices again
	for (uint32_t i = 0; i != entry_count; ++i)
		a[i] = bitfield_extract(a[i], 4, 28);
	return lehmer;
}


//! \return The number of different tuple indices that can be encoded by the
//!		given codec.
static inline uint32_t get_tuple_index_count(blend_attribute_codec_t codec, uint32_t entry_count) {
	uint32_t tuple_index_count = codec.payload_value_count_over_factorial;
	tuple_index_count *= factorial(entry_count);
	// Divide by the extra value counts
	for (uint32_t i = 0; i != entry_count; ++i)
		tuple_index_count /= codec.extra_value_counts[i];
	return tuple_index_count;
}


/*! Encodes blend weights and a tuple index for the table of bone indices into
	up to 64 bits.
	\param weights Non-negative blend weights sorted in ascending order. Their
		sum must be one.
	\param tuple_index An index into a table of blend index tuples.
	\param codec Parameters for the coding scheme. Should be a compile-time
		constant.
	\return The code for the weights and the tuple index. If the codec takes
		less than 64 bits, the most significant bits of result[1] are 0. Same
		for result[0] if it takes less than 32 bits.*/
static inline uint64_t compress_blend_attributes(float weights[MAX_ENTRY_COUNT + 1], uint32_t tuple_index, blend_attribute_codec_t codec) {
	uint32_t entry_count = codec.entry_count;
	// We mostly do work per weight
	float prefix = 0.0f;
	uint32_t quantized[MAX_ENTRY_COUNT];
	uint32_t extra[MAX_ENTRY_COUNT];
	for (uint32_t i = 0; i != entry_count; ++i) {
		// Transform the weights so that they fill the unit cube (once we
		// permutate them)
		float cube_weight = (entry_count + 1 - i) * weights[i] + prefix;
		prefix += weights[i];
		// Quantize to the requested precision and apply an offset to ensure a
		// strict ordering
		const uint32_t value_count = (codec.weight_value_count - entry_count) * codec.extra_value_counts[i];
		const uint32_t offset = (i + 1) * codec.extra_value_counts[i] - 1;
		uint32_t combined = (uint32_t) (((float) value_count) * cube_weight + (offset + 0.5f));
		// Separate the extra precision from the base precision
		quantized[i] = combined / codec.extra_value_counts[i];
		extra[i] = combined - codec.extra_value_counts[i] * quantized[i];
	}
	// Pack the tuple index and all extra precision into one integer
	uint32_t payload = tuple_index;
	for (uint32_t i = 0; i != entry_count; ++i)
		payload = payload * codec.extra_value_counts[i] + extra[i];
	// Grab high bits from the payload to store them separately
	uint32_t payload_quotient = payload / factorial(entry_count);
	uint32_t lehmer = payload - factorial(entry_count) * payload_quotient;
	// Encode part of the payload into the permutation
	permute_inverse(quantized, lehmer, entry_count);
	// Pack it all into uint64_t
	uint64_t code = payload_quotient;
	for (uint32_t i = 0; i != entry_count; ++i)
		code = code * codec.weight_value_count + quantized[i];
	return code;
}


/*! Inverse of compress_blend_attributes().
	\param out_weights The decoded weights (with some rounding error).
	\param out_valid Set to true if the given code may have been produced by
		compress_blend_attributes(). Useful for debugging, ignore it otherwise.
	\param code Output of compress_blend_attributes().
	\param codec Parameters for the coding scheme. Should be a compile-time
		constant.
	\return The tuple index that was passed to compress_blend_attributes().*/
static inline uint32_t decompress_blend_attributes(float* out_weights, int* out_valid, uint64_t code, blend_attribute_codec_t codec) {
	uint32_t entry_count = codec.entry_count;
	// Grab the quantized weights in reverse order
	uint32_t quantized[MAX_ENTRY_COUNT];
	for (uint32_t i = 0; i != entry_count; ++i) {
		// Extract the appropriate fractional bits and shift the code
		uint64_t old_code = code;
		code /= codec.weight_value_count;
		quantized[entry_count - 1 - i] = (uint32_t) (old_code - code * codec.weight_value_count);
	}
	// Grab the parts of the payload, which are not packed into the permutation
	uint32_t payload_quotient = (uint32_t) code;
	// Recover the Lehmer code while sorting the weights
	uint32_t lehmer = stable_sort(quantized, entry_count);
	// Assemble the payload
	uint32_t payload = payload_quotient * factorial(entry_count) + lehmer;
	// Extract extra bits from the payload
	uint32_t extra[MAX_ENTRY_COUNT];
	for (uint32_t i = 0; i != entry_count; ++i) {
		uint32_t old_payload = payload;
		payload /= codec.extra_value_counts[entry_count - 1 - i];
		extra[entry_count - 1 - i] = old_payload - payload * codec.extra_value_counts[entry_count - 1 - i];
	}
	// What remains of the payload is the tuple index
	uint32_t tuple_index = payload;
	// Check if the code is legal (no two values are identical and all values
	// are in range)
	(*out_valid) = 1;
	for (uint32_t i = 0; i != entry_count - 1; ++i)
		(*out_valid) = ((*out_valid) && quantized[i] != quantized[i + 1]);
	for (uint32_t i = 0; i != entry_count; ++i) {
		uint32_t combined = quantized[i] * codec.extra_value_counts[i] + extra[i];
		(*out_valid) = ((*out_valid)
			&& combined >= (i + 1) * codec.extra_value_counts[i] - 1
			&& combined < (codec.weight_value_count + i + 1 - entry_count) * codec.extra_value_counts[i]);
	}
	(*out_valid) = (out_valid && payload_quotient < codec.payload_value_count_over_factorial);
	// Dequantize the weights and undo the transform
	float prefix = 0.0f;
	for (uint32_t i = 0; i != entry_count; ++i) {
		// Dequantize to get weights in the simplex that fills the cube
		uint32_t combined = quantized[i] * codec.extra_value_counts[i] + extra[i];
		combined -= codec.extra_value_counts[i] * (i + 1) - 1;
		const float spacing = 1.0f / (float) ((codec.weight_value_count - entry_count) * codec.extra_value_counts[i]);
		float cube_weight = spacing * (float) combined;
		// Transform back to common blend weights
		out_weights[i] = (1.0f / (entry_count + 1 - i)) * cube_weight - prefix;
		prefix += (1.0f / ((entry_count + 1 - i) * (entry_count - i))) * cube_weight;
	}
	// It is not immediately obvious, but prefix is the sum of all weights
	// except the greatest one now
	out_weights[entry_count] = 1.0f - prefix;
	return tuple_index;
}
